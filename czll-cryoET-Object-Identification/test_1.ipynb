{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps_path = '/kaggle/input/czii-cryoet-dependencies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r /kaggle/input/czii-cryoet-dependencies/asciitree-0.3.3/ asciitree-0.3.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip wheel asciitree-0.3.3/asciitree-0.3.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asciitree-0.3.3-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installがされたかの確認\n",
    "!pip show monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    AsDiscrete,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定された次元を完全に覆うために最小限の重複でパッチの開始位置を計算する\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Calculate the starting positions of patches along a single dimension\n",
    "    with minimal overlap to cover the entire dimension.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dimension_size : int\n",
    "        Size of the dimension\n",
    "    patch_size : int\n",
    "        Size of the patch in this dimension\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[int]\n",
    "        List of starting positions for patches\n",
    "    \"\"\"\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "\n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "\n",
    "    # ここのコードはいらない気もするが一用残しておく\n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "\n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "\n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "\n",
    "    return positions\n",
    "\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    \"\"\"\n",
    "    Extract 3D patches from multiple arrays with minimal overlap to cover the entire array.\n",
    "    複数の3D配列から最小限の重複を持つバッチを抽出し、全体をカバーします\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : List[np.ndarray]\n",
    "        List of input arrays, each with shape (m, n, l)\n",
    "        抽出する立方体パッチのサイズ(a x a)\n",
    "    patch_size : int\n",
    "        Size of cubic patches (a x a x a)\n",
    "        抽出する立方体パッチサイズ\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    patches : List[np.ndarray]\n",
    "        List of all patches from all input arrays\n",
    "        全ての入力配列からちゅしゅつされたパッチのリスト\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        List of starting coordinates (x, y, z) for each patch\n",
    "        各パッチの開始位置\n",
    "    \"\"\"\n",
    "    # 入力が非空のリストであることを確認\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "\n",
    "    # 全ての配列が同じ形状を持つ配列であることを確認\n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "\n",
    "    # パッチサイズが各次元の最小サイズより小さいことを確認\n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = [] # 抽出されたパッチを格納するリスト\n",
    "    coordinates = [] # 各パッチの開始座標を格納するリスト\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    # 各次元に対するパッチの開始位置を計算\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    # 各配列からパッチを抽出\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    # 配列からパッチを切り出し\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates # パッチのリストと座標のリストを返す\n",
    "\n",
    "\n",
    "# 分割されたパッチとその開始座標から元の3D配列を再構築する\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct array from patches.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patches : List[np.ndarray]\n",
    "        List of patches to reconstruct from\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        Starting coordinates for each patch\n",
    "    original_shape : Tuple[int, int, int]\n",
    "        Shape of the original array\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Reconstructed array\n",
    "    \"\"\"\n",
    "    # 原始配列を再構築するためのゼロ配列を作成\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "\n",
    "    # パッチのサイズを取得(立方体パッチとして最初の次元のみを使用)\n",
    "    patch_size = patches[0].shape[0]\n",
    "\n",
    "    # 各パッチとその開始座標を順に処理\n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        # 再構築配列の対応する位置にパッチを配置\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch # パッチの値で上書き\n",
    "\n",
    "    # 再構築された配列を返す\n",
    "    return reconstructed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 辞書をデータフレームに変換\n",
    "def dict_to_df(coords_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, coords in coords_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "\n",
    "    # Concatenate all coordinates\n",
    "    # すべての座標を連結\n",
    "    # .vstack()は、配列を垂直方向に連結する\n",
    "    all_coords = np.vstack(all_coords)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'experiment' : experiment_name,\n",
    "        'particle_type' : all_labels,\n",
    "        'x' : all_coords[:, 0],\n",
    "        'y' : all_coords[:, 1],\n",
    "        'z' : all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
    "TEST_DATA_DIR = \"/kaggle/input/czii-cryo-et-object-identification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = ['TS_5_4', 'TS_69_2', 'TS_6_6', 'TS_73_6', 'TS_86_3', 'TS_99_9']\n",
    "valid_names = ['TS_6_4']\n",
    "\n",
    "train_files = []\n",
    "valid_files = []\n",
    "\n",
    "for name in train_names:\n",
    "    # 画像データとラベルデータを読み込む\n",
    "    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{name}.npy\")\n",
    "    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{name}.npy\")\n",
    "\n",
    "    train_files.append({\"image\": image, \"label\": label})\n",
    "\n",
    "for name in valid_names:\n",
    "    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{name}.npy\")\n",
    "    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{name}.npy\")\n",
    "\n",
    "    valid_files.append({\"image\": image, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-random transforms to be cached\n",
    "\n",
    "# トランスフォームの定義\n",
    "non_random_transforms = Compose([\n",
    "    # チャンネル次元を先頭に配置。画像とラベルのデータに適用\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    # 画像データの強度値を正規化(標準化)する\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    # 画像とラベルのオリエンテーションを\"RAS\"(右、前、上)に統一する\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "# データの前処理結果をキャッシュする\n",
    "raw_train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "my_num_samples = 16\n",
    "train_batch_size = 1\n",
    "\n",
    "# Random transforms to be applied during training\n",
    "# トレーニング中に適用されるランダムなトランスフォームの定義\n",
    "random_transforms = Compose([\n",
    "    # ラベルのクラスごとにランダムに切り取るトランスフォーム\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[98, 98, 98], # 切り取り後の空間サイズ(深さ、高さ、幅)\n",
    "        num_samples=my_num_samples # 生成するサンプル数\n",
    "    ),\n",
    "    # 画像およびラベルを90度単位でランダムに回転させるトランスフォーム\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    # 画像およびラベルを指定した軸に沿ってランダムに反転させるトランスフォーム\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "])\n",
    "\n",
    "train_ds = Dataset(data=raw_train_ds, transform=random_transforms)\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds, # トレーニングデータセット\n",
    "    batch_size=train_batch_size, # バッチサイズ\n",
    "    shuffle=True, # データをシャッフル\n",
    "    num_workers=4, # 使用するワーカーの数\n",
    "    pin_memory=torch.cuda.is_available() # CPUが利用可能な場合、ピンメモリを使用\n",
    ")\n",
    "\n",
    "# データローダーの確認\n",
    "print(f\"Number of workers: {train_loader.num_workers}\")\n",
    "print(f\"Pin memory: {train_loader.pin_memory}\")\n",
    "print(f\"Number of samples in raw_train_ds: {len(raw_train_ds)}\")\n",
    "print(f\"Number of samples in train_ds: {len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
