{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps_path = '/kaggle/input/czii-cryoet-dependencies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r /kaggle/input/czii-cryoet-dependencies/asciitree-0.3.3/ asciitree-0.3.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip wheel asciitree-0.3.3/asciitree-0.3.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asciitree-0.3.3-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installがされたかの確認\n",
    "!pip show monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    AsDiscrete,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定された次元を完全に覆うために最小限の重複でパッチの開始位置を計算する\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Calculate the starting positions of patches along a single dimension\n",
    "    with minimal overlap to cover the entire dimension.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dimension_size : int\n",
    "        Size of the dimension\n",
    "    patch_size : int\n",
    "        Size of the patch in this dimension\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[int]\n",
    "        List of starting positions for patches\n",
    "    \"\"\"\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "\n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "\n",
    "    # ここのコードはいらない気もするが一用残しておく\n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "\n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "\n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "\n",
    "    return positions\n",
    "\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    \"\"\"\n",
    "    Extract 3D patches from multiple arrays with minimal overlap to cover the entire array.\n",
    "    複数の3D配列から最小限の重複を持つバッチを抽出し、全体をカバーします\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : List[np.ndarray]\n",
    "        List of input arrays, each with shape (m, n, l)\n",
    "        抽出する立方体パッチのサイズ(a x a)\n",
    "    patch_size : int\n",
    "        Size of cubic patches (a x a x a)\n",
    "        抽出する立方体パッチサイズ\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    patches : List[np.ndarray]\n",
    "        List of all patches from all input arrays\n",
    "        全ての入力配列からちゅしゅつされたパッチのリスト\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        List of starting coordinates (x, y, z) for each patch\n",
    "        各パッチの開始位置\n",
    "    \"\"\"\n",
    "    # 入力が非空のリストであることを確認\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "\n",
    "    # 全ての配列が同じ形状を持つ配列であることを確認\n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "\n",
    "    # パッチサイズが各次元の最小サイズより小さいことを確認\n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = [] # 抽出されたパッチを格納するリスト\n",
    "    coordinates = [] # 各パッチの開始座標を格納するリスト\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    # 各次元に対するパッチの開始位置を計算\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    # 各配列からパッチを抽出\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    # 配列からパッチを切り出し\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates # パッチのリストと座標のリストを返す\n",
    "\n",
    "\n",
    "# 分割されたパッチとその開始座標から元の3D配列を再構築する\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct array from patches.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patches : List[np.ndarray]\n",
    "        List of patches to reconstruct from\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        Starting coordinates for each patch\n",
    "    original_shape : Tuple[int, int, int]\n",
    "        Shape of the original array\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Reconstructed array\n",
    "    \"\"\"\n",
    "    # 原始配列を再構築するためのゼロ配列を作成\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "\n",
    "    # パッチのサイズを取得(立方体パッチとして最初の次元のみを使用)\n",
    "    patch_size = patches[0].shape[0]\n",
    "\n",
    "    # 各パッチとその開始座標を順に処理\n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        # 再構築配列の対応する位置にパッチを配置\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch # パッチの値で上書き\n",
    "\n",
    "    # 再構築された配列を返す\n",
    "    return reconstructed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 辞書をデータフレームに変換\n",
    "def dict_to_df(coords_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, coords in coords_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "\n",
    "    # Concatenate all coordinates\n",
    "    # すべての座標を連結\n",
    "    # .vstack()は、配列を垂直方向に連結する\n",
    "    all_coords = np.vstack(all_coords)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'experiment' : experiment_name,\n",
    "        'particle_type' : all_labels,\n",
    "        'x' : all_coords[:, 0],\n",
    "        'y' : all_coords[:, 1],\n",
    "        'z' : all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
    "TEST_DATA_DIR = \"/kaggle/input/czii-cryo-et-object-identification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = ['TS_5_4', 'TS_69_2', 'TS_6_6', 'TS_73_6', 'TS_86_3', 'TS_99_9']\n",
    "valid_names = ['TS_6_4']\n",
    "\n",
    "train_files = []\n",
    "valid_files = []\n",
    "\n",
    "for name in train_names:\n",
    "    # 画像データとラベルデータを読み込む\n",
    "    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{name}.npy\")\n",
    "    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{name}.npy\")\n",
    "\n",
    "    train_files.append({\"image\": image, \"label\": label})\n",
    "\n",
    "for name in valid_names:\n",
    "    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{name}.npy\")\n",
    "    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{name}.npy\")\n",
    "\n",
    "    valid_files.append({\"image\": image, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-random transforms to be cached\n",
    "\n",
    "# トランスフォームの定義\n",
    "non_random_transforms = Compose([\n",
    "    # チャンネル次元を先頭に配置。画像とラベルのデータに適用\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    # 画像データの強度値を正規化(標準化)する\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    # 画像とラベルのオリエンテーションを\"RAS\"(右、前、上)に統一する\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "# データの前処理結果をキャッシュする\n",
    "raw_train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "my_num_samples = 16\n",
    "train_batch_size = 1\n",
    "\n",
    "# Random transforms to be applied during training\n",
    "# トレーニング中に適用されるランダムなトランスフォームの定義\n",
    "random_transforms = Compose([\n",
    "    # ラベルのクラスごとにランダムに切り取るトランスフォーム\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[98, 98, 98], # 切り取り後の空間サイズ(深さ、高さ、幅)\n",
    "        num_samples=my_num_samples # 生成するサンプル数\n",
    "    ),\n",
    "    # 画像およびラベルを90度単位でランダムに回転させるトランスフォーム\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    # 画像およびラベルを指定した軸に沿ってランダムに反転させるトランスフォーム\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "])\n",
    "\n",
    "train_ds = Dataset(data=raw_train_ds, transform=random_transforms)\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds, # トレーニングデータセット\n",
    "    batch_size=train_batch_size, # バッチサイズ\n",
    "    shuffle=True, # データをシャッフル\n",
    "    num_workers=4, # 使用するワーカーの数\n",
    "    pin_memory=torch.cuda.is_available() # CPUが利用可能な場合、ピンメモリを使用\n",
    ")\n",
    "\n",
    "# データローダーの確認\n",
    "print(f\"Number of workers: {train_loader.num_workers}\")\n",
    "print(f\"Pin memory: {train_loader.pin_memory}\")\n",
    "print(f\"Number of samples in raw_train_ds: {len(raw_train_ds)}\")\n",
    "print(f\"Number of samples in train_ds: {len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images, val_labels = [dcts['image'] for dcts in valid_files], [dcts['label'] for dcts in valid_files]\n",
    "\n",
    "# バリデーション用の画像データとラベルデータから3Dパッチを抽出\n",
    "# パッチサイズは96、重複を最小限に抑えて抽出\n",
    "val_image_patches, _ = extract_3d_patches_minimal_overlap(val_images, 96)\n",
    "val_label_patches, _ = extract_3d_patches_minimal_overlap(val_labels, 96)\n",
    "\n",
    "val_patched_data = [{\"image\": img, \"label\": lbl} for img, lbl in zip(val_image_patches, val_label_patches)]\n",
    "\n",
    "# データの前処理結果をキャッシュする\n",
    "valid_ds = CacheDataset(data=val_patched_data, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "valid_batch_size = 16\n",
    "# DataLoader remains the same\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=valid_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"Number of workers: {valid_loader.num_workers}\")\n",
    "print(f\"Pin memory: {valid_loader.pin_memory}\")\n",
    "print(f\"Number of samples in valid_ds: {len(valid_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int = 3, # データの空間次元数(2Dなら2、3Dなら3)\n",
    "        in_channels: int = 1, # 入力データのチャンネル数 (グレースケールなら1、RGBなら3)\n",
    "        out_channels: int = 7, # 出力データのチャンネル数(クラス数)\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80), # 各層のフィルター数\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1), #各層のストライド (畳み込みの移動幅)\n",
    "        num_res_units: int = 1, # 各層における残差ユニットの数\n",
    "        lr: float=1e-3 # 学習率\n",
    "        ):\n",
    "\n",
    "        super().__init__() # 親クラスの初期化メソッドを呼び出す\n",
    "        self.save_hyperparameters() # ハイパーパラメータを保存\n",
    "        # UNetモデルのインスタンスを作成\n",
    "        # self.save_hyperparameters()を呼び出すことで、__init__メソッドメソッドに渡されたハイパーパラメータが自動的にself.hparamsに保存される\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "        # TverskyLossを損失関数として初期化する\n",
    "        # include_background=True: 背景クラスも損失計算に含める\n",
    "        # to_onehot_y=True: ターゲットラベルをワンホット形式に変換する\n",
    "        # softmax=True: モデルの出力にソフトマックス関数を適用して確率値に変換する\n",
    "        self.loss_fn = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True) # softmax= True for multiclass\n",
    "        # DiceMetricをメトリック関数として初期化する\n",
    "        # 正確なセグメンテーションの評価に使用される\n",
    "        # reduction=\"mean\": バッチ内の各サンプルの平均を取る\n",
    "        # ignore_empty=True: 対象オブジェクトがない場合評価から除外する\n",
    "        self.metric_fn = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "        self.train_loss = 0 # トレーニング中の累計損失\n",
    "        self.val_metric = 0 # バリデーション中の累計メトリック(評価指標)\n",
    "        self.num_train_batch = 0 # トレーニングで使用したバッチ数のカウンタ\n",
    "        self.num_val_batch = 0 # バリデーションで使用したバッチ数のカウンタ\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) # モデルの出力を返す\n",
    "\n",
    "    # トレーニングステップ\n",
    "    # トレーニング中に1バッチ分の処理を行う\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # バッチから画像とラベルを抽出する: 'image'キーから入力データxを、'label'キーからターゲットデータ(正解ラベル)yを取得\n",
    "        x, y = batch['image'], batch['label']\n",
    "        # self(x) を呼び出すと、定義済みの forward() メソッド経由でモデルに入力 x を渡し、\n",
    "        # 予測結果 y_hat を得る（内部では self.model(x) が実行される）\n",
    "        # LightningModuleの内部でcallが実装されており、self(x)と記述すると内部的にself.forward(x)が呼び出される\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y) # 予測結果y_hatと正解ラベルyの損失を計算\n",
    "        self.train_loss += loss # トレーニング中の累計損失に加算\n",
    "        self.num_train_batch += 1 # トレーニングで使用したバッチ数をカウント\n",
    "        torch.cuda.empty_cache() # GPUの未使用キャッシュを解放して、メモリ使用効率を向上させる\n",
    "        return loss\n",
    "\n",
    "    # 1エポックのトレーニングが終了したときに呼び出される\n",
    "    def on_train_epoch_end(self):\n",
    "        loss_per_epoch = self.train_loss / self.num_train_batch # 1エポックの平均損失を計算\n",
    "        self.log('train_loss', loss_per_epoch, prog_bar=True) # トレーニング損失をログに記録\n",
    "        self.train_loss = 0\n",
    "        self.num_train_batch = 0\n",
    "\n",
    "    # バリデーションステップ\n",
    "    # バリデーションデータに対して1バッチ分の評価処理を実行する\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # バリデーション時は勾配を計算しないため、torch.no_grad()コンテキストを使用する\n",
    "        with torch.no_grad(): # This ensures that gradients are not stored in memory\n",
    "            # バッチから入力画像xと正解ラベルyを抽出する\n",
    "            x, y = batch['image'], batch['label']\n",
    "            y_hat = self(x)\n",
    "\n",
    "            # decollate_batch() でバッチ内の各サンプルに分割し、AsDiscreteトランスフォームを適用して予測結果を one-hot形式に変換する\n",
    "            # argmax=Trueを指定することで、クラスごとのスコアから最大値を持つクラスを選択\n",
    "            metric_val_outputs = [AsDiscrete(argmax=True, to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y_hat)]\n",
    "            # decollate_batch()で正解ラベルを分割し、AsDiscreteトランスフォームでone-hot形式に変換する\n",
    "            metric_val_labels = [AsDiscrete(to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y)]\n",
    "\n",
    "            # compute metric for current iteration\n",
    "            # 現在のバッチに対して評価指標(DiceMetric)を計算するため、変換後の予測値とラベルを渡す\n",
    "            self.metric_fn(y_pred=metric_val_outputs, y=metric_val_labels)\n",
    "            # aggregate()メソッドでバッチ内の評価指標を\"mean_batch\"単位で集約し、1バッチ全体の平均を算出する\n",
    "            metrics = self.metric_fn.aggregate(reduction=\"mean_batch\")\n",
    "            # 集約した評価指標の全体平均を計算(ここでは全クラスの平均値を求めている)\n",
    "            val_metric = torch.mean(metrics) # I used mean over all particle species as the metric. This can be explored.\n",
    "            # 累積バリデーション評価指標に現在の評価値を加算\n",
    "            self.val_metric += val_metric\n",
    "            # バリデーションで使用したバッチ数をカウント\n",
    "            self.num_val_batch += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # 現在のバリデーションステップの評価指標を辞書形式で返す\n",
    "        return {'val_metric': val_metric}\n",
    "\n",
    "    # 1エポック毎のバリデーションプロセスの最後に実行される処理\n",
    "    def on_validation_epoch_end(self):\n",
    "        metric_per_epoch = self.val_metric / self.num_val_batch\n",
    "        self.log('val_metric', metric_per_epoch, prog_bar=True, sync_dist=False) # sync_dist=True for distributed training\n",
    "        self.val_metric = 0\n",
    "        self.num_val_batch = 0\n",
    "\n",
    "    # configure_optimizers()メソッドは、最適化アルゴリズムを定義するために使用される\n",
    "    def configure_optimizers(self):\n",
    "        # torch.optim.AdamWを使用してAdamWを用いてパラメータを最適化するオプティマイザを生成する\n",
    "        # self.parameters()によって、このモデル内の全てのパラメータ(重みやバイアス)が最適化対象となる\n",
    "        # 学習率は、self.hparams.lrで指定された値を使用する\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)\n",
    "num_res_units = 1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchの内部でfloat32マトリックス乗算を実行する際の計算精度を'neduyn'に設定\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Check if CUDA is available and then count the GPUs\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "else:\n",
    "    print(\"No GPU available. Running on CPU.\")\n",
    "devices = list(range(num_gpus))\n",
    "print(devices)\n",
    "\n",
    "# PyTorch Lighting の Trainer オブジェクトを初期化して、トレーニングの各種設定を行う\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs, # トレーニングするエポックの最大数\n",
    "    #strategy=\"ddp_notebook\",\n",
    "    accelerator=\"gpu\", # トレーニングでGPUを使用することを指定\n",
    "    devices=[0], # 使用するGPUのインデックスをリスト形式で指定(ここでは0番目のGPUを使用)\n",
    "    num_nodes=1, # 分散トレーニングを行う場合のノード数(ここでは単一ノード)\n",
    "    log_every_n_steps=10, # 10ステップ毎にログ情報を出力\n",
    "    enable_progress_bar=True, # 進捗バーを表示\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
